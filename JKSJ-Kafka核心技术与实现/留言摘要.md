+ 不从follower读几个原因：1，kafka的分区已经让读是从多个broker读从而负载均衡，不是MySQL的主从，压力都在主上；2，kafka保存的数据和数据库的性质有实质的区别就是数据具有消费的概念，是流数据，kafka是消息队列，所以消费需要位移，而数据库是实体数据不存在这个概念，如果从kafka的follower读，消费端offset控制更复杂；3，生产者来说，kafka可以通过配置来控制是否等待follower对消息确认的，如果从上面读，也需要所有的follower都确认了才可以回复生产者，造成性能下降，如果follower出问题了也不好处理。

+ 之前我们搭建一套kafka集群就不知道怎么去衡量，我再问一个相关问题，我个人觉得kafka会出现丢数据情况，比如某个分区的leader挂了，在切换选举到另外副本为leader时，这个副本还没同步之前的leader数据，这样数据就丢了。
  + 对于producer而言，如果在乎数据持久性，那么应该设置acks=all，这样当出现你说的这个情况时，producer会被显式通知消息发送失败，从而可以重试。

+ auto.leader.rebalance.enable 关于这个参数的设置，我有一点不同的意见，官网说的是如果某个broker挂了，那分布在他上的leader副本就会自动切换到其他活着的broker上，但是挂掉的broker重启之后，集群并不会将他之前的leader副本再切换回来，这样就会使其他broker上leader副本数较多，而该broker上无leader副本（无新主题创建），从而造成负载不均衡的情况。这时我们可以通过 kafka-preferred-replica-election.sh 脚本来重新平衡集群中的leader副本。但是我们配置这个参数为true的话，controller角色就会每五分钟（默认）检查一下集群不平衡的状态，进而重新平衡leader副本。

+ 修改 Topic 级 max.message.bytes，还要考虑以下两个还要修改 Broker的 replica.fetch.max.bytes 保证复制正常消费还要修改配置 fetch.message.max.bytes

+ kafka认为写入成功是指写入页缓存成功还是数据刷到磁盘成功算成功呢？
  + 写入到页缓存即认为成功。如果在flush之前机器就宕机了，的确这条数据在broker上就算丢失了。producer端表现如何取决于acks的设定。如果是acks=1而恰恰是leader broker在flush前宕机，那么的确有可能消息就丢失了，而且producer端不会重发——因为它认为是成功了。

+ 之前做车辆实时定位(汽车每10s上传一次报文)显示的时候，发现地图显示车辆会突然退回去，开始排查怀疑是后端处理的逻辑问题导致的，但是后台保证了一台车只被一个线程处理，理论上不会出现这种情况；于是猜测是不是程序接收到消息的时候时间序就已经乱了，查阅了kafka相关资料，发现kafka同一个topic是无法保证数据的顺序性的，但是同一个partition中的数据是有顺序的；根据这个查看了接入端的代码(也就是kafka的生产者)，发现是按照kafka的默认分区策略(topic有10个分区，3个副本)发送的；于是将此处发送策略改为按照key(车辆VIN码)进行分区，后面车辆的定位显示就正常了。

+ 感谢老师的分享，对于按消息键保序策略有一个疑问，假如我现在的业务数据定义了三个key，但是这三个key对应的消息生产速率不一致，按照老师上面的示意图展示的是，特定的key只会存储在特定的一个分区中，那岂不是牺牲了拓展性么，如果其中一个key的生产速率非常大，而另外2个key没那么大，却会一直占用分区，不会造成分区的空间浪费吗？还是我理解的有问题吗？希望老师解答一下，谢谢
  + 是有这样的问题。所以其实在生产环境中用key做逻辑区分并不太常见。如果不同key速率相差很大，可以考虑使用不同的topic

+ 老师，我见到有网友提问，说是消费者出现reblance的情况时。key-ordering策略可能会导致消费了“因“，reblance之后，无法消费 “果“。您给出的建议是，显示设置consumer端参数partition.assignment.strategy。这个设置。是不是只要使用了key保序策略，就一定要设置上呢？消费过程中出现reblance是很正常的啊
  + 嗯嗯，可能我没说清楚。如你说所rebalance是非常常见，如果再要求消费时消息有明确前后关系，这个就很复杂了。常见的做法是单分区来保证前后关系，但是这可能不符合很多使用场景。我给出了另一个建议，就是设置partition.assignment.strategy=Sticky，这是因为Sticky算法会最大化保证消费分区方案的不变更。假设你的因果消息都有相同的key，那么结合Sticky算法有可能保证即使出现rebalance，要消费的分区依然有原来的consumer负责。

+ 遇到一个线上问题：消息经常堆积起来，不能消费了，重启服务就能继续消费了。
  + 1.生产速度大于消费速度，这样可以适当增加分区，增加consumer数量，提升消费TPS；2. consumer消费性能低，查一下是否有很重的消费逻辑（比如拿到消息后写HDFS或HBASE这种逻辑就挺重的），看看是否可以优化consumer TPS；3. 确保consumer端没有因为异常而导致消费hang住; 4. 如果你使用的是消费者组，确保没有频繁地发生rebalance

+ Producer 通过 metadata.max.age.ms定期更新元数据，在连接多个broker的情况下，producer是如何决定向哪个broker发起该请求？
  + 向它认为当前负载最少的节点发送请求，所谓负载最少就是指未完成请求数最少的broker。producer的InFlightsRequests中维护着每个broker的等待回复消息的队列，所有队列中等待回复的消息数最少的这个队列所对应的broker就是负载最小的，因为等待数量越少说明broker处理速度越快，负载越小。

+ 消费组中的消费者个数如果超过topic的分区数，就会有消费者消费不到数据。但如果是同一个消费组里的两个消费者通过assign方法订阅了同一个TopicPartition，是不是会有一个消费者不能消费到消息？
  + 如果使用assign，则表明该consumer是独立consumer（standalone consumer），它不属于任何消费者组。独立consumer可以订阅任何分区，彼此之间也没有关系，即两个独立consumer可以订阅并消费相同的分区

+ 会不会存在这样一个情况：一个consumer正在消费一个分区的一条消息，还没有消费完，发生了rebalance(加入了一个consumer)，从而导致这条消息没有消费成功，rebalance后，另一个consumer又把这条消息消费一遍
  + 太可能出现这种情况了，尤其是结合flink的时候，每次rebalance后，flink作业可能就挂了。
  + 简单的搞可以自己实现ConsumerRebalanceListener在订阅主题的时候加入，内部实现在rebalance发生的时候保存或者提交当前消费位点，这样rebalance之后可以做相应的位点恢复，比如从保存的位点开始消费或者从上一次rebalance提交的位点继续消费。

+ 1、传统消息引擎的弊端：传统的消息引擎主要有2种模式：点对点模式 和 发布/订阅模式。但是不管是点对点模式，还是发布/订阅模式，队列发消息的能力是一定的：即某队列发完积压的所有消息的时间是有上限的，最短的时间是：消息数量*发送单个消息的时间。并且，传统消息引擎的“弹性”比较差，如果是消息引擎主动推送消息，很有可能会造成消费者端积压了很多的消息，那么，这和消息引擎的初衷“削峰填谷”是相违背的。如果是消费者主动拉取消息，可能造成多个消费者抢一条消息的情况。另一个方面是，传统消息队列的容错性比较差。消息发送完成，就从队列移除了，没有办法重新消费。2、Kafka是如何解决的：Kafka引入了主题，分区，消费者组，消费者，位移的概念，来解决扩展性和容错性问题。试想，如果我们要提高传统消息引擎的TPS，在计算机I/O能力一定的情况下，只能通过增加节点的方式，使得多个节点构成一个消息队列。那么对应到Kafka里面，节点就是分区，消息队列就是主题。同时引入位移的概念，解决了消费者端消息积压的问题。并且有多个消费者组成消费者组，提高消费能力。这也就解释了，为什么kafka某个主题下的单个分区只能分配给消费者组内的一个消费者。从逻辑上讲，如果分配给同组内的2个消费者，就相当于重复发送了2次消息，这是没有必要的。Kafka这么做相当于把原本"串行"的消息发送"并行"化，因此TPS大大提升。3、Kafka的缺点：缺点主要是Rebalance 过程，耗费的时间巨大，并且目前也没有什么好的解决办法，最好就是尽量减少Rebalance 的过程。最后，也不是说传统消息引擎就该淘汰了，还是得看具体的业务场景。但是在大数据处理方便，Kafka是具有优势的。

+ 之前遇到过的一个问题跟大家分享一下，原因描述不正确的地方还请大佬指正：log cleaner线程挂掉还有可能导致消费端出现：Marking Coordinator Dead！
  + 原因大概如下：log cleaner线程挂掉之后会导致磁盘上位移主题的文件越来越多（当然，大部分是过期数据，只是依旧存在），broker内存中会维护offsetMap，从名字上看这个map就是维护消费进度的，而这个map和位移主题的文件有关联，文件越来越多会导致offsetMap越来越大，甚至导致offsetMap构建失败（为什么会失败没有搞明白），offsetMap构建失败之后broker不会承认自己是coordinator。消费者组找coordinator的逻辑很简单：abs(consumer_groupName.hashCode) % __consumer_offset.partition.num对应的partition所在的broker就是这个group的coordinate，一旦这个broker的offsetMap构建失败，那么这个broker就不承认自己是这个group的coordinate，这个group的消费就无法继续进行，会出现Marking Coordinator Dead错误。此时需要删除过期的位移主题的文件（根据文件名很容易确定哪个是最新的），重启broker。重启过程中需要关注log cleaner是否会再次挂掉。

+ 在 Rebalance 过程中，所有 Consumer 实例都会停止消费，等待Rebalance的完成。这里想问的是，如果我有一个长耗时的业务逻辑需要处理，并且offset还未提交，这时候系统发生了Rebalance的话，是等待所有消费端当前消息都处理完成，再进行停止消费，并进行重新分配分区，还是说强制停止消费。如果强制停止消费的话，那么那些已经处理完成一半的数据并offset未提交的数据，势必会导致Rebalance后重新进行消费，导致数据产生重复消费。【消息的重复处理要怎么做？】
  + 之前poll的数据还是会被继续进行业务逻辑处理，若在rebalance停止消费期间offset并未进行提交，可能会造成该partition里面的同一批消息被重新分配给其他消费实例，造成重复消费问题。

+ 如何排查得知broker rebalance 过多，通过broker日志吗？
  + Coordinator所在的broker日志，如果经常发生rebalance，会有类似于"(Re)join group" 之类的日志

+ “当消息处理的总时间超过预设的 max.poll.interval.ms 参数值时，Kafka Consumer 端会抛出 CommitFailedException 异常”。
  + 其实逻辑是这样：消息处理的总时间超过预设的 max.poll.interval.ms 参数值  导致了 Rebalance‘；
rebalance导致了 partition assgined 的consumer member变了；导致原来的consumer 想要commit都没法commit 。（因为元信息,比如连的broker都变了）.