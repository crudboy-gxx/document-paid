+ 不从follower读几个原因：1，kafka的分区已经让读是从多个broker读从而负载均衡，不是MySQL的主从，压力都在主上；2，kafka保存的数据和数据库的性质有实质的区别就是数据具有消费的概念，是流数据，kafka是消息队列，所以消费需要位移，而数据库是实体数据不存在这个概念，如果从kafka的follower读，消费端offset控制更复杂；3，生产者来说，kafka可以通过配置来控制是否等待follower对消息确认的，如果从上面读，也需要所有的follower都确认了才可以回复生产者，造成性能下降，如果follower出问题了也不好处理。
+ 之前我们搭建一套kafka集群就不知道怎么去衡量，我再问一个相关问题，我个人觉得kafka会出现丢数据情况，比如某个分区的leader挂了，在切换选举到另外副本为leader时，这个副本还没同步之前的leader数据，这样数据就丢了。
  + 对于producer而言，如果在乎数据持久性，那么应该设置acks=all，这样当出现你说的这个情况时，producer会被显式通知消息发送失败，从而可以重试。
+ auto.leader.rebalance.enable 关于这个参数的设置，我有一点不同的意见，官网说的是如果某个broker挂了，那分布在他上的leader副本就会自动切换到其他活着的broker上，但是挂掉的broker重启之后，集群并不会将他之前的leader副本再切换回来，这样就会使其他broker上leader副本数较多，而该broker上无leader副本（无新主题创建），从而造成负载不均衡的情况。这时我们可以通过 kafka-preferred-replica-election.sh 脚本来重新平衡集群中的leader副本。但是我们配置这个参数为true的话，controller角色就会每五分钟（默认）检查一下集群不平衡的状态，进而重新平衡leader副本。
+ 修改 Topic 级 max.message.bytes，还要考虑以下两个还要修改 Broker的 replica.fetch.max.bytes 保证复制正常消费还要修改配置 fetch.message.max.bytes
+ kafka认为写入成功是指写入页缓存成功还是数据刷到磁盘成功算成功呢？
  + 写入到页缓存即认为成功。如果在flush之前机器就宕机了，的确这条数据在broker上就算丢失了。producer端表现如何取决于acks的设定。如果是acks=1而恰恰是leader broker在flush前宕机，那么的确有可能消息就丢失了，而且producer端不会重发——因为它认为是成功了。
+ 之前做车辆实时定位(汽车每10s上传一次报文)显示的时候，发现地图显示车辆会突然退回去，开始排查怀疑是后端处理的逻辑问题导致的，但是后台保证了一台车只被一个线程处理，理论上不会出现这种情况；于是猜测是不是程序接收到消息的时候时间序就已经乱了，查阅了kafka相关资料，发现kafka同一个topic是无法保证数据的顺序性的，但是同一个partition中的数据是有顺序的；根据这个查看了接入端的代码(也就是kafka的生产者)，发现是按照kafka的默认分区策略(topic有10个分区，3个副本)发送的；于是将此处发送策略改为按照key(车辆VIN码)进行分区，后面车辆的定位显示就正常了。
+ 感谢老师的分享，对于按消息键保序策略有一个疑问，假如我现在的业务数据定义了三个key，但是这三个key对应的消息生产速率不一致，按照老师上面的示意图展示的是，特定的key只会存储在特定的一个分区中，那岂不是牺牲了拓展性么，如果其中一个key的生产速率非常大，而另外2个key没那么大，却会一直占用分区，不会造成分区的空间浪费吗？还是我理解的有问题吗？希望老师解答一下，谢谢
  + 是有这样的问题。所以其实在生产环境中用key做逻辑区分并不太常见。如果不同key速率相差很大，可以考虑使用不同的topic
+ 老师，我见到有网友提问，说是消费者出现reblance的情况时。key-ordering策略可能会导致消费了“因“，reblance之后，无法消费 “果“。您给出的建议是，显示设置consumer端参数partition.assignment.strategy。这个设置。是不是只要使用了key保序策略，就一定要设置上呢？消费过程中出现reblance是很正常的啊
  + 嗯嗯，可能我没说清楚。如你说所rebalance是非常常见，如果再要求消费时消息有明确前后关系，这个就很复杂了。常见的做法是单分区来保证前后关系，但是这可能不符合很多使用场景。我给出了另一个建议，就是设置partition.assignment.strategy=Sticky，这是因为Sticky算法会最大化保证消费分区方案的不变更。假设你的因果消息都有相同的key，那么结合Sticky算法有可能保证即使出现rebalance，要消费的分区依然有原来的consumer负责。