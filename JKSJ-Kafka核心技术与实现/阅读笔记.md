# 入门知识 
+ kafka的记录、主题、分区、消息位移、副本、消费者位移、消费组、重平衡的基本概念，数据持久化和删除机制。
  + 只有主副本才可以对外服务。
  + kafka持久化通过消息日志来保存数据，日志是一个只能追加写的物理文件。在底层，日志又被切分成多个日志段，写满一个日志段后，kafka会切分出一个新的日志段，并将老的封存起来。后台有定时任务检查老的日志段是否可以删除，来回收磁盘空间。
+ kafka是消息引擎系统，也是一个分布式流处理平台。

# 集群部署方案
+ kafka磁盘容量规划计算方式
  + 计算方式与每条消息大小、每天的消息数、消息存储时间、备份数、是否启用压缩有关。
  + 计算下每天1一亿条1KB大小的消息，保留2分且留存2周时间。一天的总空间为1亿 * 1KB * 2/1024/1024=200GB，另外考虑索引等其他数据，预留10%空间总容量为220GB，保存两周220GB * 14=3TB，kafka支持数据压缩，假设压缩比0.75，最后规划的空间是3 * 0.75=2.25TB
+ kafka带宽规划计算方式
  + 假设机房环境是千兆网络1Gbps，业务目标是1小时内处理1TB的数据，kafka需要多少台服务器。每台机器除去其他资源，保留70%的带宽资源，也就是单台服务器最大带宽资源为700MB。通常在额外预留出2/3，即单台服务器可用带宽为700Mb/3=240Mbps。根据业务目标1TB * 1024 * 1024/3600s=290MB，转换为带宽290MB * 8=2330Mb，需要的机器数为2330/240约为10台服务器。如果需要额外复制需要在倍增。

# 集群参数
+ broker端参数：
  + 存储位置：log.dirs
  + zk：zookeeper.connect
  + broker连接：listeners、advertised.listeners
  + topic管理：auto.create.topics.enable、unclean.leader.election.enable、auto.leader.rebalance.enable
  + 数据留存：log.retention.{hours|minutes|ms}、log.retention.bytes、message.max.bytes
+ topic级别参数：
  + retention.ms、retention.bytes、max.message.bytes
+ JVM参数
  + 堆大小设为6GB；KAFKA_HEAP_OPTS、KAFKA_JVM_PERFORMANCE_OPTS
+ 操作系统参数
  + limit -n设为100w、swap不要设为1、Flash落盘时间设置大些

# 分区
+ 分区策略：Round-robin轮询、Randomness随机、key-orderging消息键（自定义）  
+ 如何保障有前后关系消息的顺序消费？
  + 一种不太好的方案就是只创建一个分区，这样消息就只能在一个分区读写，因此保证了全局的顺序性。第二种方案是根据选一个字段，保证前后存在关系的消息发送到同一个分区内，保证分区内的消息顺序。
+ 消费程序可以只消费topic中某个分区的消息。

# 压缩
+ kafka的消息层面分为两层，消息集合和消息。一个消息集合有若干条日志项，日志项才是封装消息的地方。
+ 压缩可以发生在两个地方：生产者端和Broker端。大多数情况broker获取消息后原封不动保存了，只有两个例外：
  + 1）生产者端和broker端如果制定不通的压缩方式，会导致broker解压缩和再压缩，通常表现CPU使用率飙升。
  + 2）新版本消息执行向老版本格式转换，这个过程会涉及消息的解压缩和重新压缩，并且还会丧失零拷贝的特性。
+ 因为broker存在对消息的验证，所以生产者压缩传输后，broker必须解压缩。这样减少网络传输导致处理消息慢的情况，是否有更好的方式？目前官方已经修复。

# 丢数据
+ 生产者丢数据：如果调用producer.send(msg)这个API，他是异步发送消息，立即返回，我们并不知道是否发送成功了。可能是网络抖动导致broker没有接收到，也可能是消息不合格被broker拒收。
  + 解决方法：要使用带有会通知的api，producer.send(msg,callback)，不要使用producer.send(msg)。
+ 消费者丢数据：消费端没有遵从先消费成功，之后在位移的方式，而是设置自动提交，取数据后位移。
  + 解决方法：消费端不要开启自动提交位移。
+ 最佳实践
  + 1.生产者不要使用producer.send(msg)，而要使用 producer.send(msg, callback)。
  + 2.生产者设置acks=all，表明所有副本broker都接收到消息才算已提交。
  + 3.生产者设置retrues为较大的值，他大于0能够自动重试消息发送，避免消息丢失。
  + 4.broker端设置unclean.leader.election.enable=false，设置不去精选分区leader。
  + 5.broker端设置replication.factor >= 3，保存消息份数。
  + 6.broker端设置min.insync.replicas > 1，消息至少写入多个副本才算已提交。
  + 7.broker端确保replication.factor > min.insync.replicas，推荐replication.factor = min.insync.replicas + 1，
  + 8.消费端enable.auto.commit=false，采用手动提交位移方式。

# 客户端拦截器
+ kafka可以用到客户端监控、端到端系统性能监测、消息审计等多种功能。

# TCP连接
+ java的客户端在初始化连接broker集群，所以在配置bootstrap.servers的时候，只需要配置3~4台就足够了。因为一旦连接上任意一台broker，就可以拿到整个集群的信息。
+ tcp连接还有两个地方被创建，一个是更新元数据后，一个是消息发送时。
+ tcp关闭的两种方式，一种是主动关闭，一种是kafka自动关闭。

# 精确一次处理语义 幂等性和事务
+ kafka消息保障有三种：最多一次、至少一次、精确一次。
+ 如何进行精确一次发送消息？通过幂等性和事务机制来保证。
+ 幂等性生产者，通过设置props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true)来实现。但是这个幂等性只能保证单分区上的幂等性，其次是只能实现会话上的幂等性。
+ 事务性生产者，保证不同会话、整个topic中不重复。
  + 生产者需要设置enable.idempotence=true和设置transactional.id一个有意义的名字。另外，还需要在生产者代码中做些调整，增加对事务API的调用。（如 initTransaction、beginTransaction、commitTransaction 和 abortTransaction，它们分别对应事务的初始化、事务开始、事务提交以及事务终止）
  + 使用事务性生产者可以保证在同一事务提交到kafka的要么都成功，要么都写入失败。但是这个写入失败kafka是将数据写入底层日志的，消费者还是能看到这些消息的，所以消费者要设置isolation.level的值为read_committed，表明消费者只读取事务性生产者提交成功的消息。（read_uncommitted为默认值）

# 消费者组
+ kafka通过使用消费组，实现了发布订阅模型和点对点模型，group内是点对点，group间是发布订阅。
+ 消费组实例数量怎么设定？
  + 理想情况下，消费组实例的数量应该等于订阅主题的分区总数。
+ 在消费组中消费端怎么管理offset呢？
  + 在消费组中，offset是保存在KV中，key是分区，value是该分区最新位移。在早起的版本中，offset是保存在zk中的，但是zk不适合频繁更新；在后续的版本中，将其放到kafka的内部注意_consumer_offsets主题内。
+ rebalance是一种协议，规定了消费组下所以消费者如何达成一致，来分配订阅的主题分区。
+ 何时会触发rebalance？
  + 1、组员发生变化，有消费者加入或者离开；2、订阅主题数发生变更，比如消费组可以通过正则表达式的方式订阅主题，在消费组运行过程中，主题数会变化；3、订阅主题分区数发生变更。
+ rebalance的分配策略？
  + 1、Range分配策略是面向每个主题的，首先会对同一个主题里面的分区按照序号进行排序，并把消费者线程按照字母顺序进行排序。然后用分区数除以消费者线程数量来判断每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。 
  + 2、RoundRobin策略的原理是将消费组内所有消费者以及消费者所订阅的所有topic的partition按照字典序排序，然后通过轮询算法逐个将分区以此分配给每个消费者。 使用RoundRobin分配策略时会出现两种情况： 1）如果同一消费组内，所有的消费者订阅的消息都是相同的，那么 RoundRobin 策略的分区分配会是均匀的。 2）如果同一消费者组内，所订阅的消息是不相同的，那么在执行分区分配的时候，就不是完全的轮询分配，有可能会导致分区分配的不均匀。如果某个消费者没有订阅消费组内的某个 topic，那么在分配分区的时候，此消费者将不会分配到这个 topic 的任何分区。 
  + 3、Sticky分配策略，这种分配策略是在kafka的0.11.X版本才开始引入的，是目前最复杂也是最优秀的分配策略。 Sticky分配策略的原理比较复杂，它的设计主要实现了两个目的： 1）分区的分配要尽可能的均匀； 2）分区的分配尽可能的与上次分配的保持相同。 如果这两个目的发生了冲突，优先实现第一个目的。
+ rebalance设计缺点有哪些？
  + 1、rebalance过程中消费者都会停止消费。2、目前的重分配的设计是所有消费者实例共同参与，全部重新分配所有分区，效率低。3、主题上百个的时候，重分配的过程会非常慢。

# __consumer_offsets主题
Kafka位移主题__consumer_offsets的契机与原因、它的作用、消息格式、写入的时机以及管理策略。
+ 老版本的消费者偏移量是保存在zk中，但是zk不太适合高频读写。
+ 主题的key通过 groupid+topic+分区号 来标识一个消费者。
+ 这个分区是集群中第一个消费者程序启动时创建的。分区数是50（offsets.topic.num.partitions），副本数是3（offsets.topic.replication.factor）。
+ 位移的提交分为自动和手动两种，通过enable.auto.conmmit=true表示自动定时提交位移，提交间隔由auto.commit.interval.ms来控制。
  + 自动提交的优势是省事，但也丧失了灵活性和可控性，还有一个问题就是消费端一直启动着，他会无限向主题写入数据。
+ kafka针对偏移量主题有删除策略，来避免主题无限期膨胀。kafka后台提供了线程定期巡检待compact的主题，这线程叫Log Cleaner。

# 重平衡
+ coordinator（协调者）负责为group执行rebalance以及提供位移管理和组成员管理。
+ 如何计算消费者的协调者是在那个broker上？
+ 发生rebalance的时机有三个，分别是组成员变化、订阅主题数量变化、订阅主题的分区数变化。
+ 当组内的消费者实例减少，这个是需要我们关注的。与消费者被踢出去有关的几个参数：
  + 1）session.timeout.ms，默认值为10s，消费者超过10s没有发送心跳，会被踢出群组。
  + 2）heartbeat.interval.ms，心跳频率。
  + 3）max.poll.interval.ms，默认5m，5分钟没消费完poll方法的返回的消息，消费端会主动离开群组。
+ 推荐配置：
  + 1）session.timeout.ms = 6s；heartbeat.interval.ms = 2s，要保证 Consumer 实例在被判定为“dead”之前，能够发送至少3轮的心跳请求，即session.timeout.ms >= 3 * heartbeat.interval.ms。
  + 2）针对业务，设置max.poll.interval.ms，如果业务处理是7吗，就可以将其设置为8m左右。

# 消费端位移
+ 消费者需要为分配给他的每个分区提交各自的位移记录。
+ 自动提交的机制：当开始调用poll方法时，提交上一次poll返回的所有消息，不会出现消费丢失情况，但是可能会重复消费。
+ 自动提交不会出现消息丢失的情况，但是会出现重复消费；手动提交比较灵活，但是在调用commitSync方法时，消费端会一直处于阻塞状态。
  + 为了解决阻塞问题，kafka的API提供异步方法 commitAsync，由于是异步的，提供了一个回调函数，供处理提交之后的逻辑。但是commitAsync是无法替代commitSync的，因为异步出了问题的不会自动重试。
+ 同步提交和异步提交结合的最佳实践：通常的提交使用异步提交，最后一次提交使用同步提交。
  + 利用同步提交的自动重试来规避那些瞬间错误，例如网络抖动、broker端GC等，因为这些都是短暂的，自动重试通常都能成功。利用异步提交来规避同步提交的阻塞，提升TPS。
```
try {
  while(true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
    process(records); // 处理消息
    commitAysnc(); // 使用异步提交规避阻塞
  }
} catch(Exception e) {
  handle(e); // 处理异常
} finally {
  try {
    consumer.commitSync(); // 最后一次提交使用同步阻塞式提交
  } finally {
       consumer.close();
  }
}
```
+ 对于5000条消息的情况，希望每100条就提交一次，减少错误恢复的时间。kafka的API也提供了方法
```
private Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
int count = 0;
……
while (true) {
  ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
  for (ConsumerRecord<String, String> record: records) {
    process(record);  // 处理消息
    offsets.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1)；
    if（count % 100 == 0）{
      consumer.commitAsync(offsets, null); // 回调处理逻辑是null
    }
    count++;
  }
}
```

# CommitFailedException异常
+ 提交失败原因是消费者组已经开始了rebalance过程，并且将要提交位移的分区分配给了另一个消费者实例。出现这种情况的原因是两地调用poll方法的时间间隔超出了期望的max.poll.interval.ms参数值。
  + 社区给出的解决方式：1）增加max.poll.interval.ms参数值。2）减少max.poll.records参数值。
+ 除了社区给出的两个方式，还可以通过减少下游系统一次性消费的消息总数；也可以下游使用多线程来加速消费。

# 多线程开发消费者
+ 第一种方案，消费者启动多个线程，每个线程维护专属的kafka consumer实例，负责完成的消息获取和消息处理流程。第二种方案，消费者使用单或多线程获取消息，同时创建多个线程执行消息的处理逻辑。
  + 第一种方案类似于启动多个消费者实例，但是受限于主题数扩展性差，线程自己处理消息容易超时引发rebalance。第二种方案扩展性好，并行能力强，但是难以维护分区内的消息顺序，处理链路上不易于位移提交管理，可能出现消息的重复消费。

# kafka监控
+ 监控有三种方式，第一种是kafka自带命令 “$ bin/kafka-consumer-groups.sh --bootstrap-server --describe --group”。第二种是consumerAPI。第三种是JMX监控指标。
+ lag值，指的是最新消息位移值和当前最新消费位移值的差值。lead值，消费者最新消费位移值和分区第一条消息位移值的差值。这两个值是一体的，lag值越大，lead值就越小。


# 关于高水位和LeaderEpoch的讨论
+ 水位的概念是什么？
+ kafka中高水位有什么作用？
+ 事务机制对消息可见性与高水位机制可见性之间的联系与差异？
+ 日志末端位移（LogEndOffset LEO）的概念是什么？
+ 高水位和LEO的更新机制是什么？